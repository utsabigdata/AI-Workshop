{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture - Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the course of history, convolution operation has helped accelerate science and signal processing in a variety of ways. With the advent of deep learning, computer vision researchers began exploring the use of 2D and 3D convolutional neural networks (CNNs) directly on 2D or 3D images to reduce the parameters involved with fully connected deep neural networks. With large amount of data and computation at their disposal, supervised CNN learning algorithms tackled problems which were almost impossible to generalize in the past decade.\n",
    "\n",
    "CNNs are impressive feature extractors, extracting features heirarchically from the training images during the learning process. First few layers close to the input data learns kernels related to high contrast points, edges, and lines. Layers further in the network learns to map these primitive kernels together to understand countours and other shapes. This heirarchical way of learning by representation enables complex pattern recognition that was impossible using traditional signal processing and machine learning algorithms.\n",
    "\n",
    "In this notebook, we will learn how to define CNNs in PyTorch and build some functions to help us along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import itertools\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as Functional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "flatten = itertools.chain.from_iterable\n",
    "\n",
    "# Some helper functions\n",
    "\n",
    "def plot_loss(loss_as_list):\n",
    "    \"\"\"\n",
    "    Plot the loss curve from a list of loss terms.\n",
    "    \"\"\"\n",
    "    plt.plot(loss_as_list, 'k')\n",
    "    _ = plt.title(\"Loss Curve\")\n",
    "    _ = plt.xlabel(\"Epochs\")\n",
    "    _ = plt.ylabel(\"Loss\")\n",
    "    \n",
    "def get_classification_results(model, loader):\n",
    "    \"\"\"\n",
    "    Print the accuracy of a trained model.\n",
    "    Loss: Cross Entropy\n",
    "    \"\"\"\n",
    "    correct, total = 0, 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for xs, ts in test_loader:\n",
    "        zs = model(xs) # do forward pass\n",
    "        pred = zs.max(1, keepdim=True)[1] # get the index of the max logit\n",
    "        correct += pred.eq(ts.view_as(pred)).sum().item() # count equal values\n",
    "        total += int(ts.shape[0]) # get total values\n",
    "\n",
    "        predictions.append(pred)\n",
    "        true_labels.append(ts)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    conf_matrix = confusion_matrix(list(flatten(true_labels)), list(flatten(predictions)))\n",
    "    cl_report = classification_report(list(flatten(true_labels)), list(flatten(predictions)), digits=4)\n",
    "    \n",
    "    print(\"Classification Report\")\n",
    "    print(cl_report)\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "N_train = 64\n",
    "N_test = 256\n",
    "\n",
    "# We will use torch.utils.data.DataLoader to wrap our dataset.\n",
    "# This provides easier batching, GPU support, etc.\n",
    "# Calling torchvision.datasets.MNIST() will download and format the MNIST\n",
    "# dataset with the transforms we specify. Here, in the transforms we first convert\n",
    "# the image to PyTorch tensor, and then normalize the image based on a given mean\n",
    "# and standard deviation. Normalizing the image does: image = (image - mean) / std.\n",
    "# We shuffle the data as well by defining shuffle=True.\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../Datasets/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=N_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../Datasets/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=N_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset = enumerate(test_loader)\n",
    "batch_idx, (one_batch_of_test_subset_x, one_batch_of_test_subset_y) = next(test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0\n",
    "plt.imshow(one_batch_of_test_subset_x[i][0], cmap='gray', interpolation='none')\n",
    "_ = plt.title(\"Ground Truth: {}\".format(one_batch_of_test_subset_y[i]))\n",
    "number7 = one_batch_of_test_subset_x[i][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks - API level discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has implementations for 1D, 2D, and 3D convolutions. Perhaps, in this lecture, the main focus will be on [Conv2D](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) function from Pytorch. The class attributes are as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class torch.nn.Conv2d(\n",
    "    in_channels: int, \n",
    "    out_channels: int, \n",
    "    kernel_size: Union[T, Tuple[T, T]], \n",
    "    stride: Union[T, Tuple[T, T]] = 1, \n",
    "    padding: Union[T, Tuple[T, T]] = 0, \n",
    "    dilation: Union[T, Tuple[T, T]] = 1, \n",
    "    groups: int = 1, \n",
    "    bias: bool = True, \n",
    "    padding_mode: str = 'zeros')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good time to introduce a new way to define our neural network models. So far, we have been using the `nn.Sequential` APIs to define our network in a systematic sequential way. But, as we move into more complex neural network architectures, we might be limited by sequential nature of data and gradient flow. We will redefine our network as a `class` with an `__init__` method and `forward` function to carry out the forward pass. Let's look at our new model which we name `CNN_A`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/CNN_A.png \"CNN_A architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_A(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_A, self).__init__()\n",
    "        # We can define the arguments of each layer in the __init__ method.\n",
    "        # __init__ method will be called everytime we create an object of this class.\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(1600, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This is the forward pass function.\n",
    "        # See how we can save the activation outputs of each layer into a variable.\n",
    "        # In this case, we are saving the output of each layer\n",
    "        # to the same variable and replacing the value every time\n",
    "        # before sending to a new layers.\n",
    "        # conv layer 1\n",
    "        x = self.conv1(x)\n",
    "        # maxpooling 2D\n",
    "        x = Functional.max_pool2d(x, 2)\n",
    "        # ReLU non-linearity\n",
    "        x = Functional.relu(x)\n",
    "        # conv layer 2\n",
    "        x = self.conv2(x)\n",
    "        # maxpooling 2D\n",
    "        x = Functional.max_pool2d(x, 2)\n",
    "        # ReLU non-linearity\n",
    "        x = Functional.relu(x)\n",
    "        # Dropout\n",
    "        x = self.dropout1(x)\n",
    "        # Flatten\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # FC1\n",
    "        x = self.fc1(x)\n",
    "        x = Functional.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # FC2\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        #FC3\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_A(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1600, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CNN_A.forward of CNN_A(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1600, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we loose the ability to see the model structure by printing the variable. But the `forward` function is very readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary(model):\n",
    "    \"\"\"\n",
    "    Returns the number of parameters (trainable and total) of a PyTorch model.\n",
    "    \"\"\"\n",
    "    print(\"Trainable parameter variables: {}\\nTotal number of parameters: {}\\nTotal number of trainable parameters: {}\".format(\n",
    "        len(list(model.parameters())),\n",
    "        sum(p.numel() for p in model.parameters()),\n",
    "        sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameter variables: 10\n",
      "Total number of parameters: 232650\n",
      "Total number of trainable parameters: 232650\n"
     ]
    }
   ],
   "source": [
    "model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output shapes of convolution layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{Wâˆ’K+2P}{S} +1$$\n",
    "\n",
    "- W is the input volume\n",
    "- K is the kernel size\n",
    "- P is the amount of padding\n",
    "- S is the stride size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/convolution_basic.gif \"Convolution Operation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of convolved activations:  3\n"
     ]
    }
   ],
   "source": [
    "W = 5\n",
    "K = 3\n",
    "P = 0\n",
    "S = 1\n",
    "\n",
    "out_shape = ((W-K+2*P)//S) + 1\n",
    "print(\"Shape of convolved activations: \", out_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution in the RGB domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/convolution.gif \"Convolution Operation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of convolved activations:  3\n"
     ]
    }
   ],
   "source": [
    "W = 5\n",
    "K = 3\n",
    "P = 1\n",
    "S = 2\n",
    "\n",
    "out_shape = ((W-K+2*P)//S) + 1\n",
    "print(\"Shape of convolved activations: \", out_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv shapes of our CNN_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of CONV1 activations:  26\n",
      "Shape of MaxPool output:  13\n",
      "Shape of CONV1 activations:  11\n",
      "Shape of MaxPool output:  5\n"
     ]
    }
   ],
   "source": [
    "W = 28\n",
    "K = 3\n",
    "P = 0\n",
    "S = 1\n",
    "\n",
    "conv1_out = ((W-K+2*P)//S) + 1\n",
    "print(\"Shape of CONV1 activations: \", conv1_out)\n",
    "\n",
    "maxpool_of_conv1 = conv1_out//2\n",
    "\n",
    "print(\"Shape of MaxPool output: \", maxpool_of_conv1)\n",
    "\n",
    "conv2_out = ((maxpool_of_conv1-K+2*P)//S) + 1\n",
    "print(\"Shape of CONV1 activations: \", conv2_out)\n",
    "\n",
    "maxpool_of_conv2 = conv2_out//2\n",
    "\n",
    "print(\"Shape of MaxPool output: \", maxpool_of_conv2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[288, 32, 18432, 64, 204800, 128, 8192, 64, 640, 10]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.numel() for p in model.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the CNN - let's hand calculate.\n",
    "> https://www.learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W_c$ = Number of weights of the Conv Layer.\n",
    "\n",
    "$B_c$ = Number of biases of the Conv Layer.\n",
    "\n",
    "$P_c$ = Number of parameters of the Conv Layer.\n",
    "\n",
    "$K$ = Size (width) of kernels used in the Conv Layer.\n",
    "\n",
    "$N$ = Number of kernels.\n",
    "\n",
    "$C$ = Number of channels of the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}  W_c &= K^2 \\times C \\times N \\\\ B_c &= N \\\\ P_c &= W_c + B_c \\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters = Weight: 288, Biases: 32\n"
     ]
    }
   ],
   "source": [
    "# Conv1 \n",
    "\n",
    "K = 3 # filter size\n",
    "C = 1 # channels from the previous layer\n",
    "N = 32 # channels in the current layer\n",
    "Wc = K**2 * C * N\n",
    "print(\"Parameters = Weight: {}, Biases: {}\".format(Wc, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters = Weight: 18432, Biases: 64\n"
     ]
    }
   ],
   "source": [
    "# Conv2\n",
    "\n",
    "K = 3\n",
    "C = 32\n",
    "N = 64\n",
    "Wc = K**2 * C * N\n",
    "print(\"Parameters = Weight: {}, Biases: {}\".format(Wc, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters between Conv and FC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W_{cf}$ = Number of weights of a FC Layer which is connected to a Conv Layer.\n",
    "\n",
    "$B_{cf}$ = Number of biases of a FC Layer which is connected to a Conv Layer.\n",
    "\n",
    "$O$ = Size (width) of the output image of the previous Conv Layer.\n",
    "\n",
    "$N$ = Number of kernels in the previous Conv Layer.\n",
    "\n",
    "$F$ = Number of neurons in the FC Layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*} W_{cf} &= O^2 \\times N \\times F \\\\ B_{cf} &= F \\\\ P_{cf} &= W_{cf} + B_{cf} \\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters = Weight: 204800, Biases: 128\n"
     ]
    }
   ],
   "source": [
    "# Flatten - FC1\n",
    "\n",
    "O = 5 # maxpool output\n",
    "N = 64 # number of channels\n",
    "F = 128 # number of nodes in fully connected layer\n",
    "Wcf = O**2 * N * F\n",
    "print(\"Parameters = Weight: {}, Biases: {}\".format(Wcf, F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters between FC and FC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W_{ff}$ = Number of weights of a FC Layer which is connected to an FC Layer.\n",
    "\n",
    "$B_{ff}$ = Number of biases of a FC Layer which is connected to an FC Layer.\n",
    "\n",
    "$P_{ff}$ = Number of parameters of a FC Layer which is connected to an FC Layer.\n",
    "\n",
    "$F$ = Number of neurons in the FC Layer.\n",
    "\n",
    "$F_{-1}$ = Number of neurons in the previous FC Layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*} W_{ff} &= F_{-1} \\times F \\\\  B_{ff} &= F \\\\ P_{ff} &= W_{ff} + B_{ff}   \\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters = Weight: 8192, Biases: 64\n"
     ]
    }
   ],
   "source": [
    "# Flatten - FC2\n",
    "\n",
    "Fm1 = 128 # number of nodes in the previous fully connected layer\n",
    "F = 64 # number of nodes in fully connected layer\n",
    "Wff = Fm1 * F\n",
    "print(\"Parameters = Weight: {}, Biases: {}\".format(Wff, F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters = Weight: 640, Biases: 10\n"
     ]
    }
   ],
   "source": [
    "# Flatten - FC3\n",
    "\n",
    "Fm1 = 64 # number of nodes in the previous fully connected layer\n",
    "F = 10 # number of nodes in fully connected layer\n",
    "Wff = Fm1 * F\n",
    "print(\"Parameters = Weight: {}, Biases: {}\".format(Wff, F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse the training code from Assignment 1 and remove the line of code where we flatten the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, NUM_EPOCHS, train_loader, device):\n",
    "    \"\"\"\n",
    "    A function to train the neural network model.\n",
    "    \"\"\"\n",
    "    # \n",
    "    loss_fn = nn.CrossEntropyLoss() # also called criterion sometimes.\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9) #SGD - Stochastic Gradient Descent\n",
    "    start = time.time()\n",
    "    loss_as_list = []\n",
    "\n",
    "    for EPOCH in range(NUM_EPOCHS):\n",
    "        running_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "\n",
    "            # Training pass\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(images.to(device)) #forward pass\n",
    "            \n",
    "            loss = loss_fn(output, labels.to(device))\n",
    "            loss_as_list.append(loss)\n",
    "\n",
    "            #This is where the model learns by backpropagating\n",
    "            loss.backward()\n",
    "\n",
    "            #And optimizes its weights here\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            print(\"Epoch {} - Training loss: {}\".format(EPOCH, running_loss/len(train_loader)))\n",
    "\n",
    "    print(\"\\nTraining Time (in minutes) =\",(time.time()-start)/60)\n",
    "    return(loss_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.5414844282718101\n",
      "Epoch 1 - Training loss: 0.16635377452508218\n",
      "Epoch 2 - Training loss: 0.12366752741909993\n",
      "Epoch 3 - Training loss: 0.10201915525503631\n",
      "\n",
      "Training Time (in minutes) = 1.166442827383677\n"
     ]
    }
   ],
   "source": [
    "# Hyper Parameters\n",
    "lr = 0.003 # learning rate\n",
    "NUM_EPOCHS = 4\n",
    "\n",
    "loss_values = train(model, NUM_EPOCHS, train_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.. \n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'cnn_a_model.pt')\n",
    "print(\"Model saved.. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the saved model\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('cnn_a_model.pt')\n",
    "print(\"Loaded the saved model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9815    0.9765    0.9790       980\n",
      "           1     0.9850    0.9868    0.9859      1135\n",
      "           2     0.9745    0.9612    0.9678      1032\n",
      "           3     0.9732    0.9703    0.9717      1010\n",
      "           4     0.9804    0.9695    0.9749       982\n",
      "           5     0.9400    0.9843    0.9617       892\n",
      "           6     0.9915    0.9697    0.9805       958\n",
      "           7     0.9680    0.9708    0.9694      1028\n",
      "           8     0.9611    0.9641    0.9626       974\n",
      "           9     0.9526    0.9564    0.9545      1009\n",
      "\n",
      "    accuracy                         0.9710     10000\n",
      "   macro avg     0.9708    0.9710    0.9708     10000\n",
      "weighted avg     0.9712    0.9710    0.9710     10000\n",
      "\n",
      "Confusion Matrix\n",
      "[[ 957    0    3    0    3    3    3    1    4    6]\n",
      " [   0 1120    3    1    0    2    2    1    6    0]\n",
      " [   2    5  992    7    0    0    0   12   13    1]\n",
      " [   0    0    2  980    0   17    0    5    4    2]\n",
      " [   1    1    4    0  952    0    2    3    1   18]\n",
      " [   1    0    0    8    0  878    1    1    2    1]\n",
      " [   5    4    0    0    7   13  929    0    0    0]\n",
      " [   1    2   10    2    0    0    0  998    3   12]\n",
      " [   6    0    4    5    3    8    0    1  939    8]\n",
      " [   2    5    0    4    6   13    0    9    5  965]]\n"
     ]
    }
   ],
   "source": [
    "get_classification_results(model.to('cpu'), test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
